
```md
# ASL Alphabet Detection â€“ ML Module

This directory contains the **Machine Learning inference module** for real-time  
**American Sign Language (ASL) alphabet detection**.

The module is designed to be **plug-and-play** for backend/web developers and exposes
a simple prediction API that works on live webcam frames.

---

## ğŸ“ Project Structure

```

ASL_ML/
â”œâ”€â”€ asl_lstm.pth
â”œâ”€â”€ hand_landmarker.task
â”œâ”€â”€ inference.py
â”œâ”€â”€ model.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README_ML.md

````

---

## ğŸ“„ File Descriptions

### ğŸ”¹ `asl_lstm.pth`
- Trained **PyTorch LSTM model weights**
- Contains the learned parameters for ASL alphabet classification
- **Do NOT retrain or modify**
- Loaded automatically during inference

---

### ğŸ”¹ `hand_landmarker.task`
- Official **MediaPipe Hand Landmark model**
- Used to extract **21 hand landmarks (x, y, z)** per frame
- Required by MediaPipe Tasks API
- Must remain in the same directory as `inference.py`

---

### ğŸ”¹ `model.py`
- Defines the **LSTM model architecture**
- Used only to reconstruct the model during inference
- No training code is present here

Example usage:
```python
from model import ASLLSTM
````

---

### ğŸ”¹ `inference.py` 

This is the **only file the backend needs to interact with**.

#### Responsibilities:

* Loads MediaPipe hand landmark detector
* Loads trained LSTM model
* Maintains a **30-frame temporal buffer**
* Performs real-time ASL alphabet prediction

#### Public API:

```python
letter, confidence = predict(frame)
```

#### Input:

* `frame`: OpenCV **BGR image** (`numpy.ndarray`)
* Source: webcam or video stream

#### Output:

* `letter`:

  * `'A'` to `'Z'` â†’ detected ASL alphabet
  * `None` â†’ no prediction yet or no hand detected
* `confidence`:

  * Float value between `0.0` and `1.0`

---

### ğŸ”¹ `requirements.txt`

Python dependencies required to run this module:

```
torch
opencv-python
mediapipe
numpy
```

Install with:

```bash
pip install -r requirements.txt
```

---

## ğŸš€ How to Use (Backend Integration)

### 1ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 2ï¸âƒ£ Import Prediction Function

```python
from inference import predict
```

---

### 3ï¸âƒ£ Call `predict()` on Each Frame

```python
while True:
    ret, frame = webcam.read()
    letter, confidence = predict(frame)

    if letter is not None:
        print(letter, confidence)
```

---

## â±ï¸ Runtime Behavior (IMPORTANT)

* First ~30 frames:

  ```text
  None, 0.0
  ```
* After buffer is filled:

  ```text
  A, 0.87
  ```
* If no hand is detected:

  ```text
  None, 0.0
  ```

ğŸ‘‰ Backend should **ignore output until `letter` is not None**.

---

## ğŸ§  Model Details

* **Feature Extraction:** MediaPipe Hand Landmarks (21 Ã— 3)
* **Sequence Length:** 30 frames
* **Model:** 2-layer LSTM (PyTorch)
* **Classes:** 26 (ASL Alphabets Aâ€“Z)
* **Hand Support:** Single hand only

---

## Limitations

* Supports **ASL alphabets only (Aâ€“Z)**
* Does **not** support:

  * Words
  * Sentences
  * Grammar
* Only single-hand gestures are supported

---

## ğŸ¯ Intended Usage

This module serves as the **ML backend component** of a sign language detection system.

* Frontend UI
* API routing
* Deployment
  are handled **outside this module**.

---

## âœ… Developer Notes

* Call `predict(frame)` once per frame
* Do not reset internal buffers manually
* Always pass **OpenCV BGR frames**
* Designed for real-time, low-latency inference

---

## ğŸ“Œ Summary

âœ” Ready-to-use ML inference module
âœ” Real-time ASL alphabet detection
âœ” Backend-friendly API
âœ” Clean separation from frontend logic

If predictions fail, first check:

* Model file existence
* Camera frame format
* Dependency installation

---

```



## âœ… Final Confirmation

- âœ” Markdown is valid  
- âœ” Web-dev friendly  
- âœ” Explains **every file and behavior clearly**  
- âœ” Ready to ship  

You can now **zip the folder and send it** with full confidence.  
If you want, next I can:
- review your friendâ€™s backend code
- help you write a **GitHub main README**
- help you explain this project in interviews ğŸ‘Š
```
